{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports necessary modules for anomaly detection in HEP data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Load and Preprocess the data\n",
    "    - Unzip the training sample \n",
    "    - Process data in chunks\n",
    "    - Select relevant features of the data\n",
    "    - Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1PVQTx8l5Pdqws9-AIMLsPm0P8jslOz2r\n",
      "From (redirected): https://drive.google.com/uc?id=1PVQTx8l5Pdqws9-AIMLsPm0P8jslOz2r&confirm=t&uuid=8f326d89-fc43-49e9-a757-d131e20a92d3\n",
      "To: /home/hero/projects/masters/AI Models for Physics/anomaly-detection-HEP/dataset/train_sample.zip\n",
      "100%|██████████| 903M/903M [00:43<00:00, 20.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dataset/train_sample.zip'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the file ID and the output file name\n",
    "file_id = \"1PVQTx8l5Pdqws9-AIMLsPm0P8jslOz2r\"\n",
    "file_name = \"dataset/train_sample.zip\"\n",
    "\n",
    "# Construct the download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, file_name, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the sample data\n",
    "with ZipFile('dataset/train_sample.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('dataset/train_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process data in chunks\n",
    "def process_chunks(event_prefix, chunk_size=10000):\n",
    "    hits_files = [f for f in os.listdir(event_prefix) if f.endswith('-hits.csv')]\n",
    "    truth_files = [f for f in os.listdir(event_prefix) if f.endswith('-truth.csv')]\n",
    "\n",
    "    features_list = []\n",
    "\n",
    "    for hits_file, truth_file in zip(hits_files, truth_files):\n",
    "        hits_df = pd.read_csv(os.path.join(event_prefix, hits_file), usecols=['hit_id', 'x', 'y', 'z', 'volume_id', 'layer_id', 'module_id'])\n",
    "        truth_df = pd.read_csv(os.path.join(event_prefix, truth_file), usecols=['hit_id', 'particle_id', 'tpx', 'tpy', 'tpz'])\n",
    "        \n",
    "        merged_df = pd.merge(hits_df, truth_df, on='hit_id', suffixes=('_hit', '_truth'))\n",
    "\n",
    "        features = merged_df[['x', 'y', 'z', 'tpx', 'tpy', 'tpz', 'volume_id', 'layer_id', 'module_id']]\n",
    "        features_list.append(features)\n",
    "\n",
    "        # Process in chunks to reduce memory usage\n",
    "        if len(features_list) * chunk_size > 100000:  # Arbitrary limit to process in chunks\n",
    "            features_chunk = pd.concat(features_list, ignore_index=True)\n",
    "            yield features_chunk\n",
    "            features_list = []\n",
    "\n",
    "    if features_list:\n",
    "        features_chunk = pd.concat(features_list, ignore_index=True)\n",
    "        yield features_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data in chunks\n",
    "features_list = []\n",
    "for features_chunk in process_chunks('dataset/train_sample/train_100_events'):\n",
    "    features_list.append(features_chunk)\n",
    "\n",
    "# Concatenate all chunks\n",
    "all_features = pd.concat(features_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>tpx</th>\n",
       "      <th>tpy</th>\n",
       "      <th>tpz</th>\n",
       "      <th>volume_id</th>\n",
       "      <th>layer_id</th>\n",
       "      <th>module_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-91.5941</td>\n",
       "      <td>-2.51915</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.461990</td>\n",
       "      <td>-0.023119</td>\n",
       "      <td>-7.26105</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-79.3265</td>\n",
       "      <td>-7.12083</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.407713</td>\n",
       "      <td>-0.100549</td>\n",
       "      <td>-8.31968</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-78.2882</td>\n",
       "      <td>-1.67258</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.566050</td>\n",
       "      <td>-0.023969</td>\n",
       "      <td>-8.94242</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-66.8071</td>\n",
       "      <td>-11.33720</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.379724</td>\n",
       "      <td>-0.023690</td>\n",
       "      <td>-7.30623</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-78.4016</td>\n",
       "      <td>-14.57120</td>\n",
       "      <td>-1502.5</td>\n",
       "      <td>-0.756905</td>\n",
       "      <td>0.067117</td>\n",
       "      <td>-18.88480</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x         y       z       tpx       tpy       tpz  volume_id  \\\n",
       "0 -91.5941  -2.51915 -1502.5 -0.461990 -0.023119  -7.26105          7   \n",
       "1 -79.3265  -7.12083 -1502.5 -0.407713 -0.100549  -8.31968          7   \n",
       "2 -78.2882  -1.67258 -1502.5 -0.566050 -0.023969  -8.94242          7   \n",
       "3 -66.8071 -11.33720 -1502.5 -0.379724 -0.023690  -7.30623          7   \n",
       "4 -78.4016 -14.57120 -1502.5 -0.756905  0.067117 -18.88480          7   \n",
       "\n",
       "   layer_id  module_id  \n",
       "0         2          1  \n",
       "1         2          1  \n",
       "2         2          1  \n",
       "3         2          1  \n",
       "4         2          1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10350837, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and remove NaN values\n",
    "all_features = all_features.dropna()\n",
    "        \n",
    "# Remove duplicates\n",
    "all_features = all_features.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10350837, 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop headers by resetting the index and ignoring the first row\n",
    "all_features = all_features.reset_index(drop=True)\n",
    "all_features.columns = range(all_features.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3279, -0.0070, -1.4202,  ..., -1.1444, -1.1566, -0.7123],\n",
       "        [-0.2843, -0.0233, -1.4202,  ..., -1.1444, -1.1566, -0.7123],\n",
       "        [-0.2807, -0.0040, -1.4202,  ..., -1.1444, -1.1566, -0.7123],\n",
       "        ...,\n",
       "        [-3.1056,  0.4135,  2.8956,  ...,  2.3150,  1.7786, -0.5141],\n",
       "        [-3.2991,  0.2800,  2.8956,  ...,  2.3150,  1.7786, -0.5141],\n",
       "        [-3.3021,  0.2313,  2.8956,  ...,  2.3150,  1.7786, -0.5141]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(all_features)\n",
    "features_scaled = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "features_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
